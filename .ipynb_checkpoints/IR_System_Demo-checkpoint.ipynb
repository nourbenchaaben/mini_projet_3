{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Information Retrieval System Demo\n",
    "\n",
    "This notebook demonstrates the complete TF-IDF information retrieval pipeline including:\n",
    "- Document loading and preprocessing\n",
    "- TF-IDF vectorization\n",
    "- Document ranking with cosine similarity\n",
    "- Rocchio relevance feedback\n",
    "- Ablation studies\n",
    "- Performance evaluation and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP and IR\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Import project utilities\n",
    "from src.utils import load_docs, save_json, save_pickle, save_sparse_matrix\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "docs, titles = load_docs(\"data/News_Category_Dataset_v3.json\")\n",
    "\n",
    "print(f\"Total documents: {len(docs):,}\")\n",
    "print(f\"Total titles: {len(titles):,}\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(f\"Title: {titles[0]}\")\n",
    "print(f\"Text: {docs[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document lengths\n",
    "doc_lengths = [len(doc.split()) for doc in docs]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(doc_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(doc_lengths, vert=True)\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Document Length Statistics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean document length: {np.mean(doc_lengths):.1f} words\")\n",
    "print(f\"Median document length: {np.median(doc_lengths):.1f} words\")\n",
    "print(f\"Min/Max: {min(doc_lengths)} / {max(doc_lengths)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "# Fit and transform documents\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(f\"Document-term matrix shape: {X.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_):,}\")\n",
    "print(f\"Matrix sparsity: {(1 - X.nnz / (X.shape[0] * X.shape[1])) * 100:.2f}%\")\n",
    "print(f\"\\nTop 20 features by IDF:\")\n",
    "\n",
    "# Get top features by IDF\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "top_idf_idx = np.argsort(idf_scores)[-20:]\n",
    "\n",
    "for idx in reversed(top_idf_idx):\n",
    "    print(f\"  {feature_names[idx]}: {idf_scores[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_query(query, vectorizer, X, k=10):\n",
    "    \"\"\"\n",
    "    Rank documents for a given query using cosine similarity.\n",
    "    \"\"\"\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).ravel()\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Test with a sample query\n",
    "test_query = \"climate change and global warming\"\n",
    "results = rank_query(test_query, vectorizer, X, k=5)\n",
    "\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "print(\"Top 5 Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, (doc_id, score) in enumerate(results, 1):\n",
    "    print(f\"\\n{rank}. Score: {score:.4f}\")\n",
    "    print(f\"   Title: {titles[doc_id]}\")\n",
    "    print(f\"   Text: {docs[doc_id][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Interactive search function with formatted output.\n",
    "    \"\"\"\n",
    "    results = rank_query(query, vectorizer, X, k=top_k)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for rank, (doc_id, score) in enumerate(results, 1):\n",
    "        print(f\"{rank}. [Score: {score:.4f}]\")\n",
    "        print(f\"   ðŸ“° {titles[doc_id]}\")\n",
    "        print(f\"   ðŸ“ {docs[doc_id][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Try different queries\n",
    "queries = [\n",
    "    \"election results\",\n",
    "    \"earthquake Japan\",\n",
    "    \"space mission launch\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    search_documents(q, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rocchio Relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio_feedback(query, relevant_docs, non_relevant_docs, \n",
    "                     vectorizer, X, alpha=1.0, beta=0.75, gamma=0.15, k=10):\n",
    "    \"\"\"\n",
    "    Implement Rocchio relevance feedback.\n",
    "    \n",
    "    Args:\n",
    "        query: Original query string\n",
    "        relevant_docs: List of relevant document IDs\n",
    "        non_relevant_docs: List of non-relevant document IDs\n",
    "        alpha: Weight for original query\n",
    "        beta: Weight for relevant documents\n",
    "        gamma: Weight for non-relevant documents\n",
    "    \"\"\"\n",
    "    # Get original query vector\n",
    "    query_vec = vectorizer.transform([query]).toarray()\n",
    "    \n",
    "    # Get relevant and non-relevant document vectors\n",
    "    rel_vecs = X[relevant_docs].toarray() if relevant_docs else np.zeros((0, X.shape[1]))\n",
    "    non_rel_vecs = X[non_relevant_docs].toarray() if non_relevant_docs else np.zeros((0, X.shape[1]))\n",
    "    \n",
    "    # Apply Rocchio formula\n",
    "    new_query = alpha * query_vec.copy()\n",
    "    \n",
    "    if rel_vecs.shape[0] > 0:\n",
    "        new_query += beta * rel_vecs.mean(axis=0)\n",
    "    \n",
    "    if non_rel_vecs.shape[0] > 0:\n",
    "        new_query -= gamma * non_rel_vecs.mean(axis=0)\n",
    "    \n",
    "    # Compute similarities with updated query\n",
    "    similarities = (X.dot(new_query.T)).ravel()\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "# Example: Search and apply feedback\n",
    "query = \"artificial intelligence\"\n",
    "\n",
    "# Initial search\n",
    "print(\"Initial Search Results:\")\n",
    "initial_results = search_documents(query, top_k=5)\n",
    "\n",
    "# Simulate user feedback (mark first 2 as relevant, next 2 as non-relevant)\n",
    "relevant = [initial_results[0][0], initial_results[1][0]]\n",
    "non_relevant = [initial_results[2][0], initial_results[3][0]]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Applying Rocchio Feedback...\")\n",
    "print(f\"Relevant docs: {relevant}\")\n",
    "print(f\"Non-relevant docs: {non_relevant}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Apply Rocchio\n",
    "rocchio_results = rocchio_feedback(query, relevant, non_relevant, vectorizer, X, k=5)\n",
    "\n",
    "print(\"Rocchio Refined Results:\")\n",
    "for rank, (doc_id, score) in enumerate(rocchio_results, 1):\n",
    "    print(f\"{rank}. [Score: {score:.4f}]\")\n",
    "    print(f\"   ðŸ“° {titles[doc_id]}\")\n",
    "    print(f\"   ðŸ“ {docs[doc_id][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant_docs, retrieved_docs, k):\n",
    "    \"\"\"Calculate Precision@K\"\"\"\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = len(set(retrieved_k) & set(relevant_docs))\n",
    "    return relevant_retrieved / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(relevant_docs, retrieved_docs, k):\n",
    "    \"\"\"Calculate Recall@K\"\"\"\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = len(set(retrieved_k) & set(relevant_docs))\n",
    "    return relevant_retrieved / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "\n",
    "def average_precision(relevant_docs, retrieved_docs):\n",
    "    \"\"\"Calculate Average Precision\"\"\"\n",
    "    if not relevant_docs:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        if doc in relevant_docs:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / i\n",
    "    \n",
    "    return score / len(relevant_docs)\n",
    "\n",
    "# Example evaluation\n",
    "query = \"technology innovation\"\n",
    "results = rank_query(query, vectorizer, X, k=20)\n",
    "retrieved = [doc_id for doc_id, _ in results]\n",
    "\n",
    "# Simulate ground truth (in practice, this would come from manual judgments)\n",
    "np.random.seed(42)\n",
    "relevant = list(np.random.choice(retrieved[:10], size=5, replace=False))\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"Precision@5:  {precision_at_k(relevant, retrieved, 5):.3f}\")\n",
    "print(f\"Precision@10: {precision_at_k(relevant, retrieved, 10):.3f}\")\n",
    "print(f\"Recall@10:    {recall_at_k(relevant, retrieved, 10):.3f}\")\n",
    "print(f\"AP:           {average_precision(relevant, retrieved):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation Study: Impact of Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ablation results if available\n",
    "ablation_file = \"outputs/ablation/ablation_metrics.csv\"\n",
    "\n",
    "if os.path.exists(ablation_file):\n",
    "    ablation_df = pd.read_csv(ablation_file)\n",
    "    \n",
    "    print(\"Ablation Study Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ablation_df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    metrics = ['P@5', 'P@10', 'Recall@10', 'MAP']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        ablation_df.plot(x='Pipeline', y=metric, kind='bar', ax=ax, legend=False)\n",
    "        ax.set_title(f'{metric} by Pipeline', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Pipeline Configuration')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best pipeline for each metric\n",
    "    print(\"\\nBest Pipeline for Each Metric:\")\n",
    "    print(\"=\" * 80)\n",
    "    for metric in metrics:\n",
    "        best_idx = ablation_df[metric].idxmax()\n",
    "        best_pipeline = ablation_df.loc[best_idx, 'Pipeline']\n",
    "        best_score = ablation_df.loc[best_idx, metric]\n",
    "        print(f\"{metric:12s}: {best_pipeline} ({best_score:.4f})\")\n",
    "else:\n",
    "    print(f\"Ablation results not found. Run: python -m src.ablation_pipelines && python -m src.run_ablation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Baseline vs Rocchio Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results if available\n",
    "baseline_file = \"outputs/results/results.csv\"\n",
    "rocchio_file = \"outputs/results/results_rocchio.csv\"\n",
    "\n",
    "if os.path.exists(baseline_file) and os.path.exists(rocchio_file):\n",
    "    # This would require running evaluate.py first\n",
    "    # For now, show how to visualize if data exists\n",
    "    print(\"To compare Baseline vs Rocchio:\")\n",
    "    print(\"1. Run: python -m src.rank_and_eval\")\n",
    "    print(\"2. Fill judgments_template.csv with relevance judgments\")\n",
    "    print(\"3. Run: python -m src.evaluate\")\n",
    "    print(\"4. Run: python -m src.plot_results\")\n",
    "    print(\"\\nPlots will be saved in outputs/plots/\")\n",
    "else:\n",
    "    print(\"Results files not found. Follow the pipeline in the README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "\n",
    "# Create vocabulary dataframe\n",
    "vocab_df = pd.DataFrame({\n",
    "    'term': feature_names,\n",
    "    'idf': idf_scores\n",
    "}).sort_values('idf', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Discriminative Terms (Highest IDF):\")\n",
    "print(vocab_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nTop 20 Most Common Terms (Lowest IDF):\")\n",
    "print(vocab_df.tail(20).to_string(index=False))\n",
    "\n",
    "# Plot IDF distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(idf_scores, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('IDF Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of IDF Scores')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Vectorizer and Matrix (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the vectorizer and matrix for later use\n",
    "save_notebook_outputs = False  # Set to True to save\n",
    "\n",
    "if save_notebook_outputs:\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    save_pickle(vectorizer, \"outputs/tfidf_vectorizer_notebook.pkl\")\n",
    "    save_sparse_matrix(X, \"outputs/X_notebook.npz\")\n",
    "    print(\"âœ“ Saved vectorizer and matrix to outputs/\")\n",
    "else:\n",
    "    print(\"Set save_notebook_outputs=True to save vectorizer and matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. âœ… Loading and exploring the news dataset\n",
    "2. âœ… Building TF-IDF vectorizer and document-term matrix\n",
    "3. âœ… Ranking documents using cosine similarity\n",
    "4. âœ… Interactive query interface\n",
    "5. âœ… Rocchio relevance feedback implementation\n",
    "6. âœ… Evaluation metrics (Precision, Recall, AP)\n",
    "7. âœ… Ablation study visualization\n",
    "8. âœ… Vocabulary analysis\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different queries\n",
    "- Try different Rocchio parameters (alpha, beta, gamma)\n",
    "- Compare preprocessing configurations\n",
    "- Analyze query-specific performance\n",
    "- Implement additional ranking algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
